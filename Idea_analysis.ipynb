{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155292a0",
   "metadata": {},
   "source": [
    "Similar articles:\n",
    "1. [Emotion Recognition for Partial Faces Using a Feature Vector Technique](https://www.mdpi.com/1424-8220/22/12/4633)  \n",
    "**Summary:** This study addresses facial emotion recognition from partially occluded faces due to masks during the COVID-19 pandemic. The authors propose a three-step method: (1) synthetically masking input images to retain only the upper face region (eyes, eyebrows, part of the nose, and forehead), (2) extracting features using a novel rapid landmark detection technique based on an “infinity shape” model combined with Histogram of Oriented Gradients (HOG), and (3) classifying emotions using a hybrid CNN-LSTM architecture. They evaluated their approach on the CK+ and RAF-DB datasets, achieving high accuracies of 99.30% and 95.58%, respectively, outperforming existing state-of-the-art methods. The core focus was on recognizing emotions using only the visible upper facial region when the lower face is masked.\n",
    "\n",
    "2. [Mapping the emotional face. How individual face parts contribute to successful emotion recognition](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177239)  \n",
    "**Summary:** This study investigates which specific facial features human observers rely on to recognize basic emotional expressions (based on Ekman’s model). Using a dynamic masking technique with 48 movable tiles, participants revealed parts of a face until they could identify the emotion, allowing researchers to quantify the diagnostic value of each facial region. Results showed that the eyes and mouth were the most critical areas overall, with sadness and fear primarily recognized via the eyes, and disgust and happiness via the mouth. The most informative regions aligned with known Facial Action Coding System (FACS) action units. A similarity analysis revealed that expressions clustered by emotion rather than low-level visual features, and that reliance on eyes versus mouth structured a continuous psychological space of emotion recognition.\n",
    "\n",
    "3. [Staged transfer learning for multi-label half-face emotion recognition](https://link.springer.com/article/10.1186/s44147-025-00615-x)  \n",
    "**Summary:** This study proposes a deep learning approach for recognizing emotions from only half of the human face, introducing EMOFACE—a new dataset with 25 emotion labels for multi-label half-facial emotion classification—and combining it with the FER2013 dataset. Using a staged transfer learning framework with a custom ConvNet and five pre-trained models (VGG16, VGG19, DenseNet, MobileNet, ResNet), the method achieves high performance, reporting average binary accuracies of 0.9244 (training), 0.9152 (validation), and 0.9138 (testing). The research focuses on enabling accurate multi-label emotion recognition from partial facial information, with applications in affective computing, healthcare, robotics, and human–computer interaction.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
